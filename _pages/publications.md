---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---


### 2025

<ol start="17" reversed="reversed">

<li> <b>Knowledge Graph Retrieval-Augmented Generation for LLM-based Recommendation.</b> [<a href="https://arxiv.org/pdf/2501.02226">PDF</a>]<br>
Shijie Wang, Wenqi Fan, Yue Feng, <ins><b>Xinyu Ma</b></ins>, Shuaiqiang Wang, Dawei Yin
<br>
<b>Arxiv</b>
</li>
</ol>



### 2024

<ol start="16" reversed="reversed">

<li> <b>Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models.</b> [<a href="https://arxiv.org/pdf/2412.14574">PDF</a>][<a href="https://github.com/8421BCD/fullrank">Code</a>]<br>
Wenhan Liu, <ins><b>Xinyu Ma</b></ins>, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou
<br>
<b>Arxiv</b>
</li>
</ol>



<ol start="15" reversed="reversed">

<li> <b>JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework.</b> [<a href="https://arxiv.org/pdf/2410.12855">PDF</a>][<a href="https://github.com/usail-hkust/Jailjudge">Code</a>]<br>
Fan Liu, Yue Feng, Zhao Xu, Lixin Su, <ins><b>Xinyu Ma</b></ins>, Dawei Yin, Hao Liu
<br>
<b>Arxiv</b>
</li>
</ol>


<ol start="14" reversed="reversed">

<li> <b>MAIR: A Massive Benchmark for Evaluating Instructed Retrieval.</b> [<a href="https://arxiv.org/pdf/2410.10127">PDF</a>][<a href="https://github.com/sunnweiwei/Mair">Code</a>]<br>
Weiwei Sun, Zhengliang Shi, Jiulong Wu, Lingyong Yan, <ins><b>Xinyu Ma</b></ins>, Yiding Liu, Min Cao, Dawei Yin, Zhaochun Ren
<br>
<b>EMNLP'2024 Main</b>
</li>
</ol>


<ol start="13" reversed="reversed">

<li> <b>Understanding the Collapse of LLMs in Model Editing.</b> [<a href="https://arxiv.org/pdf/2406.11263">PDF</a>]<br>
Wanli Yang, Fei Sun, Jiajun Tan, <ins><b>Xinyu Ma</b></ins>, Du Su, Dawei Yin, Huawei Shen
<br>
<b>EMNLP'2024 Findings</b>
</li>
</ol>


<ol start="12" reversed="reversed">

<li> <b>The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse.</b> [<a href="https://arxiv.org/pdf/2402.09656.pdf">PDF</a>][<a href="https://github.com/WanliYoung/Collapse-in-Model-Editing">Code</a>][<a href="https://yangwl.site/collapse-in-model-editing/">Website</a>]<br>
Wanli Yang, Fei Sun, <ins><b>Xinyu Ma</b></ins>, Xun Liu, Dawei Yin, Xueqi Cheng
<br>
<b>ACL'2024 Findings</b>
</li>
</ol>


### 2023

<ol start="11" reversed="reversed">

<li> <b>Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers.</b> [<a href="https://arxiv.org/pdf/2311.01555.pdf">PDF</a>][<a href="https://github.com/sunnweiwei/RankGPT">Code</a>]<br>
Weiwei Sun, Zheng Chen, <ins><b>Xinyu Ma</b></ins>, Lingyong Yan, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren
<br>
<b>GenRec Workshop at CIKM 2023</b>
</li>
</ol>


<ol start="10" reversed="reversed">

<li> <b>Pre-training with Aspect-Content Text Mutual Prediction for Multi-Aspect Dense Retrieval.</b> [<a href="https://arxiv.org/pdf/2308.11474.pdf">PDF</a>][<a href="https://github.com/sunxiaojie99/ATTEMPT">Code</a>]<br>
Xiaojie Sun, Keping Bi, Jiafeng Guo, <ins><b>Xinyu Ma</b></ins>, Fan Yixing, Hongyu Shan, Qishen Zhang, Zhongyi Liu
<br>
<b>CIKM'2023</b>, <i>(Short Paper)</i> <br>
</li>
</ol>

<ol start="9" reversed="reversed">

<li> <b>Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent.</b> [<a href="https://arxiv.org/pdf/2208.09847.pdf">PDF</a>][<a href="https://github.com/sunnweiwei/RankGPT">Code</a>]<br>
Weiwei Sun, Lingyong Yan, <ins><b>Xinyu Ma</b></ins>, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren 
<br>
<b>EMNLP'2023</b> <i>(Full Paper)</i> <span style="color:#DC143C"> Outstanding Paper Award </span> <br>
</li>
</ol>


### 2022

<ol start="8" reversed="reversed">

<li> <b>Scattered or Connected? An Optimized Parameter-efficient Tuning Approach for Information Retrieval.</b> [<a href="https://arxiv.org/pdf/2208.09847.pdf">PDF</a>][<a href="/files/cikm2022-pet4ir.pdf">Slides</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xueqi Cheng. <br>
<b>CIKM'2022</b>, <i>(Full Paper)</i> <br>
</li>

<li> <b>A Contrastive Pre-training Approach to Discriminative Autoencoder for Dense Retrieval</b> [<a href="https://arxiv.org/pdf/2208.09846.pdf">PDF</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Ruqing Zhang, Jiafeng Guo, Yixing Fan, Xueqi Cheng. <br>
<b>CIKM'2022</b>, <i>(Short Paper)</i> <br>
</li>

</ol>


### 2021
<ol start="6" reversed="reversed">

<li> <b>Pre-training Methods in Information Retrieval.</b> [<a href="https://arxiv.org/abs/2111.13853">PDF</a>][<a href="https://github.com/ict-bigdatalab/awesome-pretrained-models-for-information-retrieval">Code</a>] <br>
Yixing Fan<sup>*</sup>, Xiaohui Xie<sup>*</sup>, Yinqiong Cai, Jia Chen, <ins><b>Xinyu Ma</b></ins>, Xiangsheng Li, Ruqing Zhang, Jiafeng Guo. (<sup>*</sup>Equal contribution) <br>
Foundations and Trends® in Information Retrieval. (I wrote Section 4 and Section 6.) <br> 
</li>

<li> <b>Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction.</b> [<a href="https://arxiv.org/abs/2204.10641">PDF</a>][<a href="https://github.com/Albert-Ma/COSTA">Code</a>][<a href="/files/costa_slides.pdf">Slides</a>][<a href="https://dl.acm.org/doi/abs/10.1145/3477495.3531772">Video</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xueqi Cheng. <br>
<b>SIGIR'2022</b>, <i>(Full Paper, Acceptance Rate = 20%)</i> <br>
</li>

</ol>


### 2020

<ol start="4" reversed="reversed">

<li> <b>B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval.</b> [<a href="https://arxiv.org/abs/2104.09791">PDF</a>][<a href="https://github.com/Albert-Ma/PROP">Code</a>][<a href="/files/bprop_slides.pdf">Slides</a>][<a href="https://www.bilibili.com/video/BV1mV411H7du/">Video</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Jiafeng Guo, Ruqing Zhang, Yixin Fan, Yingyan Li, Xueqi Cheng. <br>
<b>SIGIR'2021</b>, <i>(Full Paper, Acceptance Rate = 21%)</i> <br>
</li>

<li> <b>PROP: Pre-training with Representative Words Prediction for Ad-hoc Retrieval.</b> [<a href="https://arxiv.org/abs/2010.10137">PDF</a>][<a href="https://github.com/Albert-Ma/PROP">Code</a>][<a href="/files/prop_slides.pdf">Slides</a>][<a href="https://www.bilibili.com/video/BV1by4y1T7k7/">Video</a>] <br>
<ins><b>Xinyu Ma</b></ins>, Jiafeng Guo, Ruqing Zhang, Yixin Fan, Xiang Ji, Xueqi Cheng. <br>
<b>WSDM'2021</b>, <i>(Full Oral Paper, Acceptance Rate = 18.6%)</i> <br>
</li>

<li><b>A Linguistic Study on Relevance Modeling in Information Retrieval.</b> [<a href="https://arxiv.org/abs/2103.00956">PDF</a>][<a href="https://www.youtube.com/watch?v=7YIGMUGNP4o">Video</a>]<br>
Yixin Fan, Jiafeng Guo, <ins><b>Xinyu Ma</b></ins>, Ruqing Zhang, Yanyan Lan, Xueqi Cheng. <br>
<b>WWW'2021</b>, <i>(Full Paper, Acceptance Rate = 20.6%)</i> <br>
</li>

<li> <b>关于短文本匹配的泛化性和迁移性的研究分析(An Empirical Investigation of Generalization and Transfer in Short Text Matching).</b> [<a href="/files/crad2022-short-text-matching.pdf">PDF</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Lixin Su, Xueqi Cheng. <br>
<b>计算机研究与发展'2021</b>, <i>(CCF A 中文期刊)</i>
</li>

</ol>