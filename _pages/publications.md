---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---


### 2023

<ol start="11" reversed="reversed">

<li> <b>Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers.</b> [<a href="https://arxiv.org/pdf/2311.01555.pdf">PDF</a>][<a href="https://github.com/sunnweiwei/RankGPT">Code</a>]<br>
Weiwei Sun, Zheng Chen, <ins><b>Xinyu Ma</b></ins>, Lingyong Yan, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren
<br>
<b>Arxiv 2023</b>
</li>
</ol>


<ol start="10" reversed="reversed">

<li> <b>Pre-training with Aspect-Content Text Mutual Prediction for Multi-Aspect Dense Retrieval.</b> [<a href="https://arxiv.org/pdf/2308.11474.pdf">PDF</a>][<a href="https://github.com/sunxiaojie99/ATTEMPT">Code</a>]<br>
Xiaojie Sun, Keping Bi, Jiafeng Guo, <ins><b>Xinyu Ma</b></ins>, Fan Yixing, Hongyu Shan, Qishen Zhang, Zhongyi Liu
<br>
<b>CIKM'2023</b>, <i>(Short Paper)</i> <br>
</li>
</ol>

<ol start="9" reversed="reversed">

<li> <b>Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent.</b> [<a href="https://arxiv.org/pdf/2208.09847.pdf">PDF</a>][<a href="https://github.com/sunnweiwei/RankGPT">Code</a>]<br>
Weiwei Sun, Lingyong Yan, <ins><b>Xinyu Ma</b></ins>, Pengjie Ren, Dawei Yin, Zhaochun Ren 
<br>
<b>EMNLP'2023</b> <i>(Full Paper, Main Conference)</i> <br>
</li>
</ol>


### 2022

<ol start="8" reversed="reversed">

<li> <b>Scattered or Connected? An Optimized Parameter-efficient Tuning Approach for Information Retrieval.</b> [<a href="https://arxiv.org/pdf/2208.09847.pdf">PDF</a>][<a href="/files/cikm2022-pet4ir.pdf">Slides</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xueqi Cheng. <br>
<b>CIKM'2022</b>, <i>(Full Paper)</i> <br>
</li>

<li> <b>A Contrastive Pre-training Approach to Discriminative Autoencoder for Dense Retrieval</b> [<a href="https://arxiv.org/pdf/2208.09846.pdf">PDF</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Ruqing Zhang, Jiafeng Guo, Yixing Fan, Xueqi Cheng. <br>
<b>CIKM'2022</b>, <i>(Short Paper)</i> <br>
</li>

</ol>


### 2021
<ol start="6" reversed="reversed">

<li> <b>Pre-training Methods in Information Retrieval.</b> [<a href="https://arxiv.org/abs/2111.13853">PDF</a>][<a href="https://github.com/ict-bigdatalab/awesome-pretrained-models-for-information-retrieval">Code</a>] <br>
Yixing Fan<sup>*</sup>, Xiaohui Xie<sup>*</sup>, Yinqiong Cai, Jia Chen, <ins><b>Xinyu Ma</b></ins>, Xiangsheng Li, Ruqing Zhang, Jiafeng Guo. (<sup>*</sup>Equal contribution) <br>
Preprint. (I wrote Section 4 and Section 6.) <br> 
</li>

<li> <b>Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction.</b> [<a href="https://arxiv.org/abs/2204.10641">PDF</a>][<a href="https://github.com/Albert-Ma/COSTA">Code</a>][<a href="/files/costa_slides.pdf">Slides</a>][<a href="https://dl.acm.org/doi/abs/10.1145/3477495.3531772">Video</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xueqi Cheng. <br>
<b>SIGIR'2022</b>, <i>(Full Paper, Acceptance Rate = 20%)</i> <br>
</li>

</ol>


### 2020

<ol start="4" reversed="reversed">

<li> <b>B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval.</b> [<a href="https://arxiv.org/abs/2104.09791">PDF</a>][<a href="https://github.com/Albert-Ma/PROP">Code</a>][<a href="/files/bprop_slides.pdf">Slides</a>][<a href="https://www.bilibili.com/video/BV1mV411H7du/">Video</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Jiafeng Guo, Ruqing Zhang, Yixin Fan, Yingyan Li, Xueqi Cheng. <br>
<b>SIGIR'2021</b>, <i>(Full Paper, Acceptance Rate = 21%)</i> <br>
</li>

<li> <b>PROP: Pre-training with Representative Words Prediction for Ad-hoc Retrieval.</b> [<a href="https://arxiv.org/abs/2010.10137">PDF</a>][<a href="https://github.com/Albert-Ma/PROP">Code</a>][<a href="/files/prop_slides.pdf">Slides</a>][<a href="https://www.bilibili.com/video/BV1by4y1T7k7/">Video</a>] <br>
<ins><b>Xinyu Ma</b></ins>, Jiafeng Guo, Ruqing Zhang, Yixin Fan, Xiang Ji, Xueqi Cheng. <br>
<b>WSDM'2021</b>, <i>(Full Oral Paper, Acceptance Rate = 18.6%)</i> <br>
</li>

<li><b>A Linguistic Study on Relevance Modeling in Information Retrieval.</b> [<a href="https://arxiv.org/abs/2103.00956">PDF</a>][<a href="https://www.youtube.com/watch?v=7YIGMUGNP4o">Video</a>]<br>
Yixin Fan, Jiafeng Guo, <ins><b>Xinyu Ma</b></ins>, Ruqing Zhang, Yanyan Lan, Xueqi Cheng. <br>
<b>WWW'2021</b>, <i>(Full Paper, Acceptance Rate = 20.6%)</i> <br>
</li>

<li> <b>关于短文本匹配的泛化性和迁移性的研究分析(An Empirical Investigation of Generalization and Transfer in Short Text Matching).</b> [<a href="/files/crad2022-short-text-matching.pdf">PDF</a>]<br>
<ins><b>Xinyu Ma</b></ins>, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Lixin Su, Xueqi Cheng. <br>
<b>计算机研究与发展'2021</b>, <i>(CCF A 中文期刊)</i>
</li>

</ol>